---
title: "Lesson 6: Making predictions and testing hypotheses"
author: "Quentin D. Read"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  html_document:
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = TRUE, warning = TRUE)
```

# Learning objectives

At the end of this lesson, students will ...

- Generate predictions from a mixed model.
- Test specific hypotheses with contrasts.
- Compare multiple group means and correct for multiple comparisons.

# Estimated marginal means

Welcome to the final lesson of the mixed models in R workshop! Pat yourselves on the back for making it this far.

We are going to talk about estimated marginal means. You might know the term "least square means" or "lsmeans" if you are a SAS user. This statistical technique was pioneered by statisticians working for SAS. Estimated marginal means are the same thing as lsmeans in most cases. It is probably better to use the term estimated marginal means because the least squares method is not always used to calculate them. 

The name estimated marginal means makes sense because they are estimates from a model, **not data**. It's best to keep that in mind -- when we calculate these estimated means, they are functions of model parameters and sensitive to all the assumptions we've made in constructing the model. And remember, all models are wrong and only some of them are useful. They're called marginal because they can be calculated averaged across the margins of all other fixed and random effects in the model. 

# Note on estimated means

These estimated means are population-level predictions. It's important to note that the confidence interval or uncertainty around an estimated marginal mean is not the range of values that any individual in the population might have. 

For instance, a dataset gathered between 2015 and 2018 shows that [the population level mean of the height of an adult female in the United States](https://www.cdc.gov/nchs/data/series/sr_03/sr03-046-508.pdf) is 161.3 cm (5' 3.5") and the heights are roughly normally distributed. The 90% confidence interval around that estimate goes from 160.9 cm to 161.6 cm (5' 3.38" to 5' 3.63"). Because this mean was estimated from a large sample of 5,510 women, we are really sure we have that number pinned down to within a tiny range of uncertainty.

But if we took a random adult female from the United States and measured her height, does that mean we are 90% confident it would be between 160.9 cm and 161.6 cm? No! In fact, the 5th and 95th percentile are 149.8 cm (4'11") and 172.5 cm (5'8") -- so 90% of the time if we selected a random female we would expect her height to be between those much more widely separated values. So estimated means tell us what the most likely expected value of a random individual from the population would be, but not about the variation in the population itself. 

This is even more important to keep in mind when we are averaging the estimated means across random effects, where the subpopulations we are averaging across may be very different from each other. The estimated mean is a "construct" that may not correspond to any individual, like the proverbial household with 2.2 children.

# The emmeans package

The [**emmeans** package](https://cran.r-project.org/package=emmeans) is the workhorse of this section. It was developed by Russ Lenth, an emeritus statistics professor from the University of Iowa. I am very indebted to Russ for all his help -- even though he is retired, he somehow always manages to answer any question I post on the emmeans package's GitHub page within a few hours, and almost always fixes the problem right away. He is like the "man behind the curtain" in the Wizard of Oz -- whenever I appear to be helping ARS scientists with their stats, I am actually going to him to get the answer!

(insert goofy pictures here)

Today we're only going to have time for a brief introduction to the capabilities of **emmeans**. The [**emmeans** package page](https://cran.r-project.org/package=emmeans) has a long list of vignettes that you might find helpful if you want to get deeper into it. For example, **emmeans** also works with some of the Bayesian modeling packages in R, so you can use similar code to get the estimated means regardless of what method you used to fit the model.

# Use emmeans to estimate means

Let's revisit the fake biomass versus fertilization dataset that we worked with earlier. Reload the data and refit the model. Load the **emmeans** and **multcomp** packages along with the other ones we need.

```{r}
library(tidyverse)
library(lmerTest)
library(emmeans)
library(multcomp)

fert_biomass <- read_csv('datasets/fertilizer_biomass.csv') %>%
  mutate(treatment = factor(treatment, levels = c('control', 'low', 'high')))

fert_mm <- lmer(biomass ~ treatment + (1 | field), data = fert_biomass)
```

If you recall, we could use `summary(fert_mm)` to show the coefficients for the intercept and the two non-control treatment levels, and t-tests associated with each one. But the inference that you're often interested in is whether the means of each treatment differ from one another.

We can use the `emmeans()` function to make that comparison. The function can take many arguments, but it needs at least two. The first argument is a fitted model object. The second argument is a one-sided formula beginning with `~` that gives the variable or combination of variables for which we want to estimate the marginal means. In this model there is only one such variable, `treatment`. The result is an object which contains the estimated marginal means, which are averaged across all other fixed effects and all the random effects as well.

```{r}
fert_emm <- emmeans(fert_mm, ~ treatment)

fert_emm
```

Note that we now have estimated marginal means and a confidence interval (default 95%) for each one. The degrees of freedom are approximated, as must be the case with mixed models. (There are different methods we can choose from to approximate the degrees of freedom but we will not get into that right now.)

You can use the `plot()` function because `emmeans` objects have a built-in plotting method which will show the means and 95% confidence intervals.

```{r}
plot(fert_emm) + theme_bw()
```

We can see from the plot that the 95% confidence intervals of the means overlap; however this does not mean they are not significantly different from one another. The `contrast()` function allows us to do comparisons between means. The first argument is the `emmeans` object we created just now, and the second argument, `method`, is the type of contrast. Using `method = 'pairwise'` means compare all pairs of treatments. It takes the difference between each pair of means and calculates the associated *t*-statistic and *p*-value. It automatically adjusts the *p*-value for multiple comparisons using the Tukey adjustment.

```{r}
contrast(fert_emm, method = 'pairwise')
```

We can choose other methods of *p*-value adjustment using the `adjust` argument. In this case it makes very little difference.

```{r}
contr_bonferroni <- contrast(fert_emm, method = 'pairwise', adjust = 'bonferroni')

contr_bonferroni
```
The `contrast` objects also have a built-in plotting method using the `plot()` function. 

```{r}
plot(contr_bonferroni)
```

I like to add a dashed line at zero which helps identify the contrasts for which the 95% confidence interval does not contain zero.

```{r}
plot(contr_bonferroni) + geom_vline(xintercept = 0, linetype = 'dashed', linewidth = 1)
```

In this case, all of them are very far from 0.

If you have many pairwise contrasts, it's pretty common to summarize them with "multiple comparison letters." You can get the letters for a comparison of `emmeans` using the `cld()` function from the **multcomp** package.

```{r}
cld(fert_emm, adjust = 'bonferroni')
```

By default, you get labels `1`, `2`, `3` instead of `a`, `b`, `c`, but here is how to get the more familiar letters:

```{r}
cld(fert_emm, adjust = 'bonferroni', Letters = letters)
```

We can use other contrast methods. For instance we might only be concerned with the difference between the control and each other treatment. Specifying `method = 'trt.vs.ctrl'` means to only do contrasts between each other level and the reference level (the first level in the factor ordering). This will apply Dunnett's adjustment to the p-values which has higher power to detect differences from the control as we are ignoring differences between the non-control levels.

```{r}
contrast(fert_emm, method = 'trt.vs.ctrl')
```

# Back-transformation

In the example above, we weren't dealing with a model with a transformed response, or a GLMM with a link function. However if either of those things is true, the estimated marginal means will be output on the transformed scale and not the original data scale by default. But this is easy to fix. You can add an extra argument, `type = 'response'`, to `emmeans()`. This will tell it to present the estimated marginal means transformed back to the original data scale (known as the response scale).

Let's revisit one of the examples from lesson 5 to illustrate this, the Stirret corn borers dataset.

```{r}
data('stirret.borers', package = 'agridat')

stirret.borers <- stirret.borers %>%
  mutate(trt = factor(trt, levels = c('None', 'Early', 'Late', 'Both')))

glmm_borers <- glmer(count2 ~ trt + (1|block), data = stirret.borers, family = poisson)
```

Here are the estmated marginal means on the log scale.

```{r}
emmeans(glmm_borers, ~ trt)
```

And here they are on the response scale.

```{r}
emm_borers <- emmeans(glmm_borers, ~ trt, type = 'response')

emm_borers
```

Let's take the contrasts between each pair of treatments.

```{r}
contrast(emm_borers, method = 'pairwise', adjust = 'bonferroni')
```

Notice that the second column is now called `ratio`. This is because when you subtract two values on a log scale, that is equivalent to taking the ratio of those values when you back-transform to the original data scale. Because our count data model uses a log link function, any comparisons we make are on a ratio scale. For example, the ratio `None / Late` is 2.72, meaning that the model estimates about 2.72 times as many corn borers would be expected on a plot with no fungal spores applied, versus one that received the late fungal spore application treatment.

# Comparison of estimated marginal means versus ANOVA

- If you properly account for multiple comparisons you do not need to worry about the F-test

# Hey! What about ...

Likelihood ratio tests

# Course Recap

What have we learned?

# Exercises

TBD